{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tobia\\OneDrive\\Documents\\Code\\AppliedMachineLearning\\dataloader.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.36045039  0.97104829  0.30529895 ...  1.63679763 -0.89304986\n",
      " -0.36045039]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  \n",
      "c:\\Users\\tobia\\OneDrive\\Documents\\Code\\AppliedMachineLearning\\dataloader.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.51101585 -0.39351628 -0.17542437 ...  0.04201507 -0.22126679\n",
      "  0.11440979]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  \n",
      "c:\\Users\\tobia\\OneDrive\\Documents\\Code\\AppliedMachineLearning\\dataloader.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.33838254 -0.70352354 -0.33838254 ...  0.02675847  0.02675847\n",
      "  1.48732249]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  \n",
      "c:\\Users\\tobia\\OneDrive\\Documents\\Code\\AppliedMachineLearning\\dataloader.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.31318085 -0.31318085 -0.31318085 ... -0.31318085 -0.31318085\n",
      " -0.31318085]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  \n",
      "c:\\Users\\tobia\\OneDrive\\Documents\\Code\\AppliedMachineLearning\\dataloader.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.26907829 -0.26907829 -0.26907829 ... -0.26907829  3.37182545\n",
      "  3.37182545]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  \n",
      "c:\\Users\\tobia\\OneDrive\\Documents\\Code\\AppliedMachineLearning\\dataloader.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.57782885 -0.57782885 -0.57782885 ... -0.57782885 -0.57782885\n",
      " -0.57782885]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  \n",
      "c:\\Users\\tobia\\OneDrive\\Documents\\Code\\AppliedMachineLearning\\dataloader.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.36297413  2.75501727 -0.36297413 ... -0.36297413 -0.36297413\n",
      " -0.36297413]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  \n",
      "c:\\Users\\tobia\\OneDrive\\Documents\\Code\\AppliedMachineLearning\\dataloader.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.82655407 -0.82655407  1.20984221 ... -0.82655407  1.20984221\n",
      "  1.20984221]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  \n",
      "c:\\Users\\tobia\\OneDrive\\Documents\\Code\\AppliedMachineLearning\\dataloader.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.20521229 -0.20521229 -0.20521229 ... -0.20521229 -0.20521229\n",
      " -0.20521229]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  \n",
      "c:\\Users\\tobia\\OneDrive\\Documents\\Code\\AppliedMachineLearning\\dataloader.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.26944662 -0.26944662 -0.26944662 ... -0.26944662 -0.26944662\n",
      " -0.26944662]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  \n",
      "c:\\Users\\tobia\\OneDrive\\Documents\\Code\\AppliedMachineLearning\\dataloader.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 4.8778924  -0.20500657 -0.20500657 ...  4.8778924  -0.20500657\n",
      " -0.20500657]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  \n",
      "c:\\Users\\tobia\\OneDrive\\Documents\\Code\\AppliedMachineLearning\\dataloader.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.29278649 -0.29278649 -0.29278649 ... -0.29278649 -0.29278649\n",
      " -0.29278649]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import dataloader\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "loan = dataloader.LoanNeural()\n",
    "data = loan.get_data()\n",
    "\n",
    "train,test = dataloader.split_data_2(data)\n",
    "\n",
    "# We have low amount of class 1 samples, so we will upsample them in the training data\n",
    "class_1 = train[train['Class'] == 1]\n",
    "class_0 = train[train['Class'] == 0]\n",
    "class_1_upsampled = class_1.sample(n=len(class_0), replace\n",
    "=True, random_state=42)\n",
    "balanced_train : pd.DataFrame = pd.concat([class_0, class_1_upsampled])\n",
    "\n",
    "X_train, y_train = dataloader.split_variables_and_target(balanced_train)\n",
    "X_test, y_test = dataloader.split_variables_and_target(test)\n",
    "\n",
    "# Scale the data\n",
    "X_train = loan.scale_data(X_train)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(list(zip(X_train, y_train)), batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(list(zip(X_test, y_test)), batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69493398\n",
      "Iteration 2, loss = 0.68867758\n",
      "Iteration 3, loss = 0.68380633\n",
      "Iteration 4, loss = 0.67919390\n",
      "Iteration 5, loss = 0.67463044\n",
      "Iteration 6, loss = 0.67009055\n",
      "Iteration 7, loss = 0.66556409\n",
      "Iteration 8, loss = 0.66117964\n",
      "Iteration 9, loss = 0.65693882\n",
      "Iteration 10, loss = 0.65302481\n",
      "Iteration 11, loss = 0.64950874\n",
      "Iteration 12, loss = 0.64646542\n",
      "Iteration 13, loss = 0.64389638\n",
      "Iteration 14, loss = 0.64163370\n",
      "Iteration 15, loss = 0.63971603\n",
      "Iteration 16, loss = 0.63801915\n",
      "Iteration 17, loss = 0.63654970\n",
      "Iteration 18, loss = 0.63522624\n",
      "Iteration 19, loss = 0.63406807\n",
      "Iteration 20, loss = 0.63294767\n",
      "Iteration 21, loss = 0.63181989\n",
      "Iteration 22, loss = 0.63085066\n",
      "Iteration 23, loss = 0.62987996\n",
      "Iteration 24, loss = 0.62890580\n",
      "Iteration 25, loss = 0.62800134\n",
      "Iteration 26, loss = 0.62712108\n",
      "Iteration 27, loss = 0.62624387\n",
      "Iteration 28, loss = 0.62547980\n",
      "Iteration 29, loss = 0.62468369\n",
      "Iteration 30, loss = 0.62391444\n",
      "Iteration 31, loss = 0.62311956\n",
      "Iteration 32, loss = 0.62236910\n",
      "Iteration 33, loss = 0.62166909\n",
      "Iteration 34, loss = 0.62096112\n",
      "Iteration 35, loss = 0.62025163\n",
      "Iteration 36, loss = 0.61970800\n",
      "Iteration 37, loss = 0.61894131\n",
      "Iteration 38, loss = 0.61836102\n",
      "Iteration 39, loss = 0.61762267\n",
      "Iteration 40, loss = 0.61706682\n",
      "Iteration 41, loss = 0.61640559\n",
      "Iteration 42, loss = 0.61571178\n",
      "Iteration 43, loss = 0.61512552\n",
      "Iteration 44, loss = 0.61450478\n",
      "Iteration 45, loss = 0.61383331\n",
      "Iteration 46, loss = 0.61331771\n",
      "Iteration 47, loss = 0.61265496\n",
      "Iteration 48, loss = 0.61200484\n",
      "Iteration 49, loss = 0.61142301\n",
      "Iteration 50, loss = 0.61084806\n",
      "Iteration 51, loss = 0.61014861\n",
      "Iteration 52, loss = 0.60947484\n",
      "Iteration 53, loss = 0.60882216\n",
      "Iteration 54, loss = 0.60828582\n",
      "Iteration 55, loss = 0.60755989\n",
      "Iteration 56, loss = 0.60704961\n",
      "Iteration 57, loss = 0.60641631\n",
      "Iteration 58, loss = 0.60564095\n",
      "Iteration 59, loss = 0.60521617\n",
      "Iteration 60, loss = 0.60437882\n",
      "Iteration 61, loss = 0.60380726\n",
      "Iteration 62, loss = 0.60314899\n",
      "Iteration 63, loss = 0.60247931\n",
      "Iteration 64, loss = 0.60166512\n",
      "Iteration 65, loss = 0.60117209\n",
      "Iteration 66, loss = 0.60032392\n",
      "Iteration 67, loss = 0.59998836\n",
      "Iteration 68, loss = 0.59904536\n",
      "Iteration 69, loss = 0.59841345\n",
      "Iteration 70, loss = 0.59766228\n",
      "Iteration 71, loss = 0.59690988\n",
      "Iteration 72, loss = 0.59622700\n",
      "Iteration 73, loss = 0.59541844\n",
      "Iteration 74, loss = 0.59502947\n",
      "Iteration 75, loss = 0.59402144\n",
      "Iteration 76, loss = 0.59332859\n",
      "Iteration 77, loss = 0.59262996\n",
      "Iteration 78, loss = 0.59169833\n",
      "Iteration 79, loss = 0.59122726\n",
      "Iteration 80, loss = 0.59037543\n",
      "Iteration 81, loss = 0.58961539\n",
      "Iteration 82, loss = 0.58871395\n",
      "Iteration 83, loss = 0.58810928\n",
      "Iteration 84, loss = 0.58725022\n",
      "Iteration 85, loss = 0.58640547\n",
      "Iteration 86, loss = 0.58552588\n",
      "Iteration 87, loss = 0.58488474\n",
      "Iteration 88, loss = 0.58407891\n",
      "Iteration 89, loss = 0.58314741\n",
      "Iteration 90, loss = 0.58236403\n",
      "Iteration 91, loss = 0.58150663\n",
      "Iteration 92, loss = 0.58068597\n",
      "Iteration 93, loss = 0.57976367\n",
      "Iteration 94, loss = 0.57880440\n",
      "Iteration 95, loss = 0.57816667\n",
      "Iteration 96, loss = 0.57728634\n",
      "Iteration 97, loss = 0.57628446\n",
      "Iteration 98, loss = 0.57536938\n",
      "Iteration 99, loss = 0.57431749\n",
      "Iteration 100, loss = 0.57364107\n",
      "Iteration 101, loss = 0.57239529\n",
      "Iteration 102, loss = 0.57158116\n",
      "Iteration 103, loss = 0.57052701\n",
      "Iteration 104, loss = 0.56955796\n",
      "Iteration 105, loss = 0.56833598\n",
      "Iteration 106, loss = 0.56761585\n",
      "Iteration 107, loss = 0.56632983\n",
      "Iteration 108, loss = 0.56525504\n",
      "Iteration 109, loss = 0.56469619\n",
      "Iteration 110, loss = 0.56370976\n",
      "Iteration 111, loss = 0.56221689\n",
      "Iteration 112, loss = 0.56126539\n",
      "Iteration 113, loss = 0.56023087\n",
      "Iteration 114, loss = 0.55905867\n",
      "Iteration 115, loss = 0.55772162\n",
      "Iteration 116, loss = 0.55678448\n",
      "Iteration 117, loss = 0.55556632\n",
      "Iteration 118, loss = 0.55441226\n",
      "Iteration 119, loss = 0.55308325\n",
      "Iteration 120, loss = 0.55176064\n",
      "Iteration 121, loss = 0.55048445\n",
      "Iteration 122, loss = 0.54944462\n",
      "Iteration 123, loss = 0.54821938\n",
      "Iteration 124, loss = 0.54691226\n",
      "Iteration 125, loss = 0.54555843\n",
      "Iteration 126, loss = 0.54433915\n",
      "Iteration 127, loss = 0.54281822\n",
      "Iteration 128, loss = 0.54153191\n",
      "Iteration 129, loss = 0.54012950\n",
      "Iteration 130, loss = 0.53867133\n",
      "Iteration 131, loss = 0.53724041\n",
      "Iteration 132, loss = 0.53586570\n",
      "Iteration 133, loss = 0.53427513\n",
      "Iteration 134, loss = 0.53228346\n",
      "Iteration 135, loss = 0.53139228\n",
      "Iteration 136, loss = 0.52981972\n",
      "Iteration 137, loss = 0.52781816\n",
      "Iteration 138, loss = 0.52601143\n",
      "Iteration 139, loss = 0.52462867\n",
      "Iteration 140, loss = 0.52264772\n",
      "Iteration 141, loss = 0.52149991\n",
      "Iteration 142, loss = 0.51940155\n",
      "Iteration 143, loss = 0.51815727\n",
      "Iteration 144, loss = 0.51621552\n",
      "Iteration 145, loss = 0.51436826\n",
      "Iteration 146, loss = 0.51283075\n",
      "Iteration 147, loss = 0.51078616\n",
      "Iteration 148, loss = 0.50886222\n",
      "Iteration 149, loss = 0.50706261\n",
      "Iteration 150, loss = 0.50540760\n",
      "Iteration 151, loss = 0.50404135\n",
      "Iteration 152, loss = 0.50134694\n",
      "Iteration 153, loss = 0.50021951\n",
      "Iteration 154, loss = 0.49738773\n",
      "Iteration 155, loss = 0.49510359\n",
      "Iteration 156, loss = 0.49318643\n",
      "Iteration 157, loss = 0.49145195\n",
      "Iteration 158, loss = 0.48955187\n",
      "Iteration 159, loss = 0.48737650\n",
      "Iteration 160, loss = 0.48487827\n",
      "Iteration 161, loss = 0.48305074\n",
      "Iteration 162, loss = 0.48087146\n",
      "Iteration 163, loss = 0.47911304\n",
      "Iteration 164, loss = 0.47698019\n",
      "Iteration 165, loss = 0.47478779\n",
      "Iteration 166, loss = 0.47180608\n",
      "Iteration 167, loss = 0.46988654\n",
      "Iteration 168, loss = 0.46706830\n",
      "Iteration 169, loss = 0.46489874\n",
      "Iteration 170, loss = 0.46200724\n",
      "Iteration 171, loss = 0.46080172\n",
      "Iteration 172, loss = 0.45772041\n",
      "Iteration 173, loss = 0.45559024\n",
      "Iteration 174, loss = 0.45334858\n",
      "Iteration 175, loss = 0.45123036\n",
      "Iteration 176, loss = 0.44783102\n",
      "Iteration 177, loss = 0.44527866\n",
      "Iteration 178, loss = 0.44204637\n",
      "Iteration 179, loss = 0.43970991\n",
      "Iteration 180, loss = 0.43855712\n",
      "Iteration 181, loss = 0.43460200\n",
      "Iteration 182, loss = 0.43251284\n",
      "Iteration 183, loss = 0.42957193\n",
      "Iteration 184, loss = 0.42659317\n",
      "Iteration 185, loss = 0.42432144\n",
      "Iteration 186, loss = 0.42182850\n",
      "Iteration 187, loss = 0.41908449\n",
      "Iteration 188, loss = 0.41659093\n",
      "Iteration 189, loss = 0.41413181\n",
      "Iteration 190, loss = 0.41065604\n",
      "Iteration 191, loss = 0.40763718\n",
      "Iteration 192, loss = 0.40627605\n",
      "Iteration 193, loss = 0.40345715\n",
      "Iteration 194, loss = 0.39850878\n",
      "Iteration 195, loss = 0.39818336\n",
      "Iteration 196, loss = 0.39394706\n",
      "Iteration 197, loss = 0.39159273\n",
      "Iteration 198, loss = 0.38844451\n",
      "Iteration 199, loss = 0.38682943\n",
      "Iteration 200, loss = 0.38316935\n",
      "Iteration 201, loss = 0.38109568\n",
      "Iteration 202, loss = 0.37842321\n",
      "Iteration 203, loss = 0.37608537\n",
      "Iteration 204, loss = 0.37271414\n",
      "Iteration 205, loss = 0.36928630\n",
      "Iteration 206, loss = 0.36654638\n",
      "Iteration 207, loss = 0.36597080\n",
      "Iteration 208, loss = 0.36198720\n",
      "Iteration 209, loss = 0.36033086\n",
      "Iteration 210, loss = 0.35472821\n",
      "Iteration 211, loss = 0.35342503\n",
      "Iteration 212, loss = 0.34934966\n",
      "Iteration 213, loss = 0.34767940\n",
      "Iteration 214, loss = 0.34446031\n",
      "Iteration 215, loss = 0.34102979\n",
      "Iteration 216, loss = 0.33896212\n",
      "Iteration 217, loss = 0.33704951\n",
      "Iteration 218, loss = 0.33275953\n",
      "Iteration 219, loss = 0.33004215\n",
      "Iteration 220, loss = 0.32752646\n",
      "Iteration 221, loss = 0.32400295\n",
      "Iteration 222, loss = 0.32309432\n",
      "Iteration 223, loss = 0.32015972\n",
      "Iteration 224, loss = 0.31685876\n",
      "Iteration 225, loss = 0.31505026\n",
      "Iteration 226, loss = 0.31316364\n",
      "Iteration 227, loss = 0.30863447\n",
      "Iteration 228, loss = 0.30506547\n",
      "Iteration 229, loss = 0.30173194\n",
      "Iteration 230, loss = 0.30102978\n",
      "Iteration 231, loss = 0.29803544\n",
      "Iteration 232, loss = 0.29471178\n",
      "Iteration 233, loss = 0.29615907\n",
      "Iteration 234, loss = 0.28993648\n",
      "Iteration 235, loss = 0.28909350\n",
      "Iteration 236, loss = 0.28627617\n",
      "Iteration 237, loss = 0.28282114\n",
      "Iteration 238, loss = 0.28078783\n",
      "Iteration 239, loss = 0.27719391\n",
      "Iteration 240, loss = 0.27609702\n",
      "Iteration 241, loss = 0.27020475\n",
      "Iteration 242, loss = 0.26948020\n",
      "Iteration 243, loss = 0.26733679\n",
      "Iteration 244, loss = 0.26298924\n",
      "Iteration 245, loss = 0.26317799\n",
      "Iteration 246, loss = 0.25958222\n",
      "Iteration 247, loss = 0.25799069\n",
      "Iteration 248, loss = 0.25188578\n",
      "Iteration 249, loss = 0.25236290\n",
      "Iteration 250, loss = 0.25003291\n",
      "Iteration 251, loss = 0.24753983\n",
      "Iteration 252, loss = 0.24595720\n",
      "Iteration 253, loss = 0.24462580\n",
      "Iteration 254, loss = 0.24101635\n",
      "Iteration 255, loss = 0.23611041\n",
      "Iteration 256, loss = 0.23695560\n",
      "Iteration 257, loss = 0.23267106\n",
      "Iteration 258, loss = 0.22842760\n",
      "Iteration 259, loss = 0.22940505\n",
      "Iteration 260, loss = 0.23038390\n",
      "Iteration 261, loss = 0.22726163\n",
      "Iteration 262, loss = 0.22151860\n",
      "Iteration 263, loss = 0.21809852\n",
      "Iteration 264, loss = 0.21984799\n",
      "Iteration 265, loss = 0.21480432\n",
      "Iteration 266, loss = 0.21002313\n",
      "Iteration 267, loss = 0.21029302\n",
      "Iteration 268, loss = 0.20818510\n",
      "Iteration 269, loss = 0.20460377\n",
      "Iteration 270, loss = 0.20159299\n",
      "Iteration 271, loss = 0.19955274\n",
      "Iteration 272, loss = 0.19860739\n",
      "Iteration 273, loss = 0.19640445\n",
      "Iteration 274, loss = 0.19799436\n",
      "Iteration 275, loss = 0.19572321\n",
      "Iteration 276, loss = 0.19614196\n",
      "Iteration 277, loss = 0.19251290\n",
      "Iteration 278, loss = 0.18673224\n",
      "Iteration 279, loss = 0.18372727\n",
      "Iteration 280, loss = 0.18757068\n",
      "Iteration 281, loss = 0.17848398\n",
      "Iteration 282, loss = 0.17386936\n",
      "Iteration 283, loss = 0.18417566\n",
      "Iteration 284, loss = 0.17597852\n",
      "Iteration 285, loss = 0.17570424\n",
      "Iteration 286, loss = 0.17408810\n",
      "Iteration 287, loss = 0.17453340\n",
      "Iteration 288, loss = 0.16519798\n",
      "Iteration 289, loss = 0.16857796\n",
      "Iteration 290, loss = 0.17211257\n",
      "Iteration 291, loss = 0.16748532\n",
      "Iteration 292, loss = 0.16379221\n",
      "Iteration 293, loss = 0.15898605\n",
      "Iteration 294, loss = 0.15496946\n",
      "Iteration 295, loss = 0.16272245\n",
      "Iteration 296, loss = 0.16229839\n",
      "Iteration 297, loss = 0.15027780\n",
      "Iteration 298, loss = 0.14959169\n",
      "Iteration 299, loss = 0.15697615\n",
      "Iteration 300, loss = 0.15421049\n",
      "Iteration 301, loss = 0.14593678\n",
      "Iteration 302, loss = 0.17271234\n",
      "Iteration 303, loss = 0.16837460\n",
      "Iteration 304, loss = 0.14986218\n",
      "Iteration 305, loss = 0.15326082\n",
      "Iteration 306, loss = 0.13937043\n",
      "Iteration 307, loss = 0.14042385\n",
      "Iteration 308, loss = 0.16605988\n",
      "Iteration 309, loss = 0.16164695\n",
      "Iteration 310, loss = 0.13983294\n",
      "Iteration 311, loss = 0.14016649\n",
      "Iteration 312, loss = 0.14634314\n",
      "Iteration 313, loss = 0.13138782\n",
      "Iteration 314, loss = 0.15865234\n",
      "Iteration 315, loss = 0.13582690\n",
      "Iteration 316, loss = 0.13388559\n",
      "Iteration 317, loss = 0.13345608\n",
      "Iteration 318, loss = 0.15087405\n",
      "Iteration 319, loss = 0.12501880\n",
      "Iteration 320, loss = 0.11925869\n",
      "Iteration 321, loss = 0.12510739\n",
      "Iteration 322, loss = 0.14925400\n",
      "Iteration 323, loss = 0.15846808\n",
      "Iteration 324, loss = 0.12459695\n",
      "Iteration 325, loss = 0.11348916\n",
      "Iteration 326, loss = 0.11954379\n",
      "Iteration 327, loss = 0.16969391\n",
      "Iteration 328, loss = 0.14874256\n",
      "Iteration 329, loss = 0.12214051\n",
      "Iteration 330, loss = 0.10512290\n",
      "Iteration 331, loss = 0.10742297\n",
      "Iteration 332, loss = 0.11154683\n",
      "Iteration 333, loss = 0.11330634\n",
      "Iteration 334, loss = 0.10844394\n",
      "Iteration 335, loss = 0.19848131\n",
      "Iteration 336, loss = 0.12614924\n",
      "Iteration 337, loss = 0.11144595\n",
      "Iteration 338, loss = 0.10172960\n",
      "Iteration 339, loss = 0.09415121\n",
      "Iteration 340, loss = 0.09962460\n",
      "Iteration 341, loss = 0.09085563\n",
      "Iteration 342, loss = 0.13338774\n",
      "Iteration 343, loss = 0.11451590\n",
      "Iteration 344, loss = 0.10051519\n",
      "Iteration 345, loss = 0.09214809\n",
      "Iteration 346, loss = 0.21520981\n",
      "Iteration 347, loss = 0.11196973\n",
      "Iteration 348, loss = 0.10111013\n",
      "Iteration 349, loss = 0.12578029\n",
      "Iteration 350, loss = 0.08707019\n",
      "Iteration 351, loss = 0.09614595\n",
      "Iteration 352, loss = 0.08820388\n",
      "Iteration 353, loss = 0.09406337\n",
      "Iteration 354, loss = 0.07786388\n",
      "Iteration 355, loss = 0.08685330\n",
      "Iteration 356, loss = 0.07962245\n",
      "Iteration 357, loss = 0.08147488\n",
      "Iteration 358, loss = 0.07631152\n",
      "Iteration 359, loss = 0.07510121\n",
      "Iteration 360, loss = 0.07826427\n",
      "Iteration 361, loss = 0.12711481\n",
      "Iteration 362, loss = 0.21383162\n",
      "Iteration 363, loss = 0.07978532\n",
      "Iteration 364, loss = 0.07942074\n",
      "Iteration 365, loss = 0.06834606\n",
      "Iteration 366, loss = 0.06627778\n",
      "Iteration 367, loss = 0.14081548\n",
      "Iteration 368, loss = 0.07368932\n",
      "Iteration 369, loss = 0.06698867\n",
      "Iteration 370, loss = 0.06863039\n",
      "Iteration 371, loss = 0.06602533\n",
      "Iteration 372, loss = 0.06837646\n",
      "Iteration 373, loss = 0.06745545\n",
      "Iteration 374, loss = 0.06813193\n",
      "Iteration 375, loss = 0.06673223\n",
      "Iteration 376, loss = 0.29478802\n",
      "Iteration 377, loss = 0.08885474\n",
      "Iteration 378, loss = 0.06824678\n",
      "Iteration 379, loss = 0.06054404\n",
      "Iteration 380, loss = 0.06327468\n",
      "Iteration 381, loss = 0.05737287\n",
      "Iteration 382, loss = 0.05782990\n",
      "Iteration 383, loss = 0.05362083\n",
      "Iteration 384, loss = 0.05452007\n",
      "Iteration 385, loss = 0.05737993\n",
      "Iteration 386, loss = 0.05309156\n",
      "Iteration 387, loss = 0.05584897\n",
      "Iteration 388, loss = 0.05300075\n",
      "Iteration 389, loss = 0.05094950\n",
      "Iteration 390, loss = 0.04833282\n",
      "Iteration 391, loss = 0.04857966\n",
      "Iteration 392, loss = 0.04650392\n",
      "Iteration 393, loss = 0.05086770\n",
      "Iteration 394, loss = 0.04872360\n",
      "Iteration 395, loss = 0.04627577\n",
      "Iteration 396, loss = 0.04384483\n",
      "Iteration 397, loss = 0.04603303\n",
      "Iteration 398, loss = 0.07113003\n",
      "Iteration 399, loss = 0.62928852\n",
      "Iteration 400, loss = 0.09380870\n",
      "Iteration 401, loss = 0.06370337\n",
      "Iteration 402, loss = 0.05231742\n",
      "Iteration 403, loss = 0.04778753\n",
      "Iteration 404, loss = 0.04638500\n",
      "Iteration 405, loss = 0.04301359\n",
      "Iteration 406, loss = 0.04556658\n",
      "Iteration 407, loss = 0.04165062\n",
      "Iteration 408, loss = 0.45390412\n",
      "Iteration 409, loss = 0.08700954\n",
      "Iteration 410, loss = 0.04929484\n",
      "Iteration 411, loss = 0.04679823\n",
      "Iteration 412, loss = 0.04377061\n",
      "Iteration 413, loss = 0.04178389\n",
      "Iteration 414, loss = 0.04426551\n",
      "Iteration 415, loss = 0.04011969\n",
      "Iteration 416, loss = 0.03867624\n",
      "Iteration 417, loss = 0.04143959\n",
      "Iteration 418, loss = 0.03779392\n",
      "Iteration 419, loss = 0.03724010\n",
      "Iteration 420, loss = 0.03485652\n",
      "Iteration 421, loss = 0.03361265\n",
      "Iteration 422, loss = 0.03374576\n",
      "Iteration 423, loss = 0.03331983\n",
      "Iteration 424, loss = 0.03464609\n",
      "Iteration 425, loss = 0.03524640\n",
      "Iteration 426, loss = 0.04154183\n",
      "Iteration 427, loss = 0.03522466\n",
      "Iteration 428, loss = 0.03300289\n",
      "Iteration 429, loss = 0.03239386\n",
      "Iteration 430, loss = 0.03190127\n",
      "Iteration 431, loss = 0.03144085\n",
      "Iteration 432, loss = 0.03060804\n",
      "Iteration 433, loss = 0.03181998\n",
      "Iteration 434, loss = 0.02776422\n",
      "Iteration 435, loss = 0.02953304\n",
      "Iteration 436, loss = 0.03043278\n",
      "Iteration 437, loss = 0.03030921\n",
      "Iteration 438, loss = 0.02885230\n",
      "Iteration 439, loss = 0.02737177\n",
      "Iteration 440, loss = 0.02672217\n",
      "Iteration 441, loss = 0.02612617\n",
      "Iteration 442, loss = 0.02716603\n",
      "Iteration 443, loss = 0.02776869\n",
      "Iteration 444, loss = 0.02687709\n",
      "Iteration 445, loss = 0.02590173\n",
      "Iteration 446, loss = 0.02727531\n",
      "Iteration 447, loss = 0.02482878\n",
      "Iteration 448, loss = 0.02450803\n",
      "Iteration 449, loss = 0.02539652\n",
      "Iteration 450, loss = 0.02338384\n",
      "Iteration 451, loss = 0.02398566\n",
      "Iteration 452, loss = 0.02316204\n",
      "Iteration 453, loss = 0.02369322\n",
      "Iteration 454, loss = 0.02366086\n",
      "Iteration 455, loss = 0.02223010\n",
      "Iteration 456, loss = 0.02283133\n",
      "Iteration 457, loss = 0.02123101\n",
      "Iteration 458, loss = 0.02156702\n",
      "Iteration 459, loss = 0.02141421\n",
      "Iteration 460, loss = 0.02098168\n",
      "Iteration 461, loss = 0.02011498\n",
      "Iteration 462, loss = 0.02359072\n",
      "Iteration 463, loss = 0.01966164\n",
      "Iteration 464, loss = 0.02167103\n",
      "Iteration 465, loss = 0.01981861\n",
      "Iteration 466, loss = 0.02001686\n",
      "Iteration 467, loss = 0.02005885\n",
      "Iteration 468, loss = 0.01902666\n",
      "Iteration 469, loss = 0.01832040\n",
      "Iteration 470, loss = 0.01805662\n",
      "Iteration 471, loss = 0.01850005\n",
      "Iteration 472, loss = 0.01784770\n",
      "Iteration 473, loss = 0.01740803\n",
      "Iteration 474, loss = 0.01749094\n",
      "Iteration 475, loss = 0.01672711\n",
      "Iteration 476, loss = 0.01734716\n",
      "Iteration 477, loss = 0.01736459\n",
      "Iteration 478, loss = 0.01648912\n",
      "Iteration 479, loss = 0.01674063\n",
      "Iteration 480, loss = 0.01558208\n",
      "Iteration 481, loss = 0.01637462\n",
      "Iteration 482, loss = 0.01535591\n",
      "Iteration 483, loss = 0.01546534\n",
      "Iteration 484, loss = 0.01504709\n",
      "Iteration 485, loss = 0.01660047\n",
      "Iteration 486, loss = 0.01520206\n",
      "Iteration 487, loss = 0.01503859\n",
      "Iteration 488, loss = 0.01474766\n",
      "Iteration 489, loss = 0.86774844\n",
      "Iteration 490, loss = 0.08770782\n",
      "Iteration 491, loss = 0.04577576\n",
      "Iteration 492, loss = 0.03180020\n",
      "Iteration 493, loss = 0.02585559\n",
      "Iteration 494, loss = 0.02311170\n",
      "Iteration 495, loss = 0.01883249\n",
      "Iteration 496, loss = 0.01781322\n",
      "Iteration 497, loss = 0.01758383\n",
      "Iteration 498, loss = 0.01855267\n",
      "Iteration 499, loss = 0.01698003\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tobia\\OneDrive\\Documents\\Code\\AppliedMachineLearning\\dataloader.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-1.4327851   0.58992066 -0.48885575 ... -0.6237028  -0.3540087\n",
      "  1.12930886]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  \n",
      "c:\\Users\\tobia\\OneDrive\\Documents\\Code\\AppliedMachineLearning\\dataloader.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.23770563 -0.05930313 -0.10242651 ... -0.35383488 -0.33815803\n",
      " -0.31686164]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  \n",
      "c:\\Users\\tobia\\OneDrive\\Documents\\Code\\AppliedMachineLearning\\dataloader.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.72629691  0.72629691 -0.7303516  ... -0.2448021  -0.7303516\n",
      " -0.7303516 ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  \n",
      "c:\\Users\\tobia\\OneDrive\\Documents\\Code\\AppliedMachineLearning\\dataloader.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.30502061 -0.30502061 -0.30502061 ... -0.30502061 -0.30502061\n",
      " -0.30502061]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  \n",
      "c:\\Users\\tobia\\OneDrive\\Documents\\Code\\AppliedMachineLearning\\dataloader.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.22887308 -0.22887308 -0.22887308 ... -0.22887308 -0.22887308\n",
      " -0.22887308]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  \n",
      "c:\\Users\\tobia\\OneDrive\\Documents\\Code\\AppliedMachineLearning\\dataloader.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.52824116 -0.52824116 -0.52824116 ...  1.89307476 -0.52824116\n",
      " -0.52824116]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  \n",
      "c:\\Users\\tobia\\OneDrive\\Documents\\Code\\AppliedMachineLearning\\dataloader.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.39535627 -0.39535627  2.52936423 ... -0.39535627 -0.39535627\n",
      " -0.39535627]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  \n",
      "c:\\Users\\tobia\\OneDrive\\Documents\\Code\\AppliedMachineLearning\\dataloader.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 1.15734106  1.15734106 -0.86404953 ... -0.86404953  1.15734106\n",
      " -0.86404953]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  \n",
      "c:\\Users\\tobia\\OneDrive\\Documents\\Code\\AppliedMachineLearning\\dataloader.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.20323471 -0.20323471 -0.20323471 ... -0.20323471 -0.20323471\n",
      " -0.20323471]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  \n",
      "c:\\Users\\tobia\\OneDrive\\Documents\\Code\\AppliedMachineLearning\\dataloader.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.27311789 -0.27311789 -0.27311789 ... -0.27311789 -0.27311789\n",
      " -0.27311789]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  \n",
      "c:\\Users\\tobia\\OneDrive\\Documents\\Code\\AppliedMachineLearning\\dataloader.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.20323471 -0.20323471 -0.20323471 ... -0.20323471 -0.20323471\n",
      " -0.20323471]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  \n",
      "c:\\Users\\tobia\\OneDrive\\Documents\\Code\\AppliedMachineLearning\\dataloader.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.2753182  -0.2753182  -0.2753182  ... -0.2753182  -0.2753182\n",
      "  3.63216087]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Create an instance of the MLPClassifier\n",
    "model = MLPClassifier(hidden_layer_sizes=(100, 100, 100, 100), activation='relu', solver='sgd', verbose=True, max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(loan.scale_data(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19624217118997914"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.sum() / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7406054279749478\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8558    0\n",
       "4629    0\n",
       "1383    1\n",
       "8142    0\n",
       "1768    0\n",
       "       ..\n",
       "1017    1\n",
       "6755    0\n",
       "3114    0\n",
       "7902    0\n",
       "1599    0\n",
       "Name: Class, Length: 1916, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
