{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tobia\\Documents\\School\\4th Semester\\Applied Machine Leaning and Big Data\\ML-Credit-Card-Fraud\\dataloader.py:60: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.68940724 -0.50947658 -1.04231384 ... -1.04231384 -0.37626727\n",
      " -0.24305796]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  data.iloc[:, 1:] = self.scaler.fit_transform(data.iloc[:, 1:])\n",
      "c:\\Users\\tobia\\Documents\\School\\4th Semester\\Applied Machine Leaning and Big Data\\ML-Credit-Card-Fraud\\dataloader.py:60: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.01265214 -0.4624939  -0.36331635 ... -0.46372066 -0.28998546\n",
      " -0.46372066]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  data.iloc[:, 1:] = self.scaler.fit_transform(data.iloc[:, 1:])\n",
      "c:\\Users\\tobia\\Documents\\School\\4th Semester\\Applied Machine Leaning and Big Data\\ML-Credit-Card-Fraud\\dataloader.py:60: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.49428169 -0.49428169 -0.49428169 ...  1.1544275  -0.82402353\n",
      " -0.82402353]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  data.iloc[:, 1:] = self.scaler.fit_transform(data.iloc[:, 1:])\n",
      "c:\\Users\\tobia\\Documents\\School\\4th Semester\\Applied Machine Leaning and Big Data\\ML-Credit-Card-Fraud\\dataloader.py:60: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.30884336 -0.30884336  1.28773789 ... -0.30884336 -0.30884336\n",
      "  1.28773789]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  data.iloc[:, 1:] = self.scaler.fit_transform(data.iloc[:, 1:])\n",
      "c:\\Users\\tobia\\Documents\\School\\4th Semester\\Applied Machine Leaning and Big Data\\ML-Credit-Card-Fraud\\dataloader.py:60: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.24924445 -0.24924445 -0.24924445 ... -0.24924445 -0.24924445\n",
      " -0.24924445]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  data.iloc[:, 1:] = self.scaler.fit_transform(data.iloc[:, 1:])\n",
      "c:\\Users\\tobia\\Documents\\School\\4th Semester\\Applied Machine Leaning and Big Data\\ML-Credit-Card-Fraud\\dataloader.py:60: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.51168772  1.95431697 -0.51168772 ... -0.51168772 -0.51168772\n",
      " -0.51168772]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  data.iloc[:, 1:] = self.scaler.fit_transform(data.iloc[:, 1:])\n",
      "c:\\Users\\tobia\\Documents\\School\\4th Semester\\Applied Machine Leaning and Big Data\\ML-Credit-Card-Fraud\\dataloader.py:60: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 1.72588965 -0.57941132 -0.57941132 ... -0.57941132 -0.57941132\n",
      " -0.57941132]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  data.iloc[:, 1:] = self.scaler.fit_transform(data.iloc[:, 1:])\n",
      "c:\\Users\\tobia\\Documents\\School\\4th Semester\\Applied Machine Leaning and Big Data\\ML-Credit-Card-Fraud\\dataloader.py:60: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.38687589 -0.38687589 -0.38687589 ... -0.38687589 -0.38687589\n",
      " -0.38687589]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  data.iloc[:, 1:] = self.scaler.fit_transform(data.iloc[:, 1:])\n",
      "c:\\Users\\tobia\\Documents\\School\\4th Semester\\Applied Machine Leaning and Big Data\\ML-Credit-Card-Fraud\\dataloader.py:60: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.81638273 -0.81638273  1.22491567 ...  1.22491567  1.22491567\n",
      " -0.81638273]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  data.iloc[:, 1:] = self.scaler.fit_transform(data.iloc[:, 1:])\n",
      "c:\\Users\\tobia\\Documents\\School\\4th Semester\\Applied Machine Leaning and Big Data\\ML-Credit-Card-Fraud\\dataloader.py:60: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.20366116  4.91011646 -0.20366116 ... -0.20366116 -0.20366116\n",
      "  4.91011646]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  data.iloc[:, 1:] = self.scaler.fit_transform(data.iloc[:, 1:])\n",
      "c:\\Users\\tobia\\Documents\\School\\4th Semester\\Applied Machine Leaning and Big Data\\ML-Credit-Card-Fraud\\dataloader.py:60: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.26204871 -0.26204871 -0.26204871 ... -0.26204871 -0.26204871\n",
      " -0.26204871]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  data.iloc[:, 1:] = self.scaler.fit_transform(data.iloc[:, 1:])\n",
      "c:\\Users\\tobia\\Documents\\School\\4th Semester\\Applied Machine Leaning and Big Data\\ML-Credit-Card-Fraud\\dataloader.py:60: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.20720023 -0.20720023 -0.20720023 ... -0.20720023 -0.20720023\n",
      " -0.20720023]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  data.iloc[:, 1:] = self.scaler.fit_transform(data.iloc[:, 1:])\n",
      "c:\\Users\\tobia\\Documents\\School\\4th Semester\\Applied Machine Leaning and Big Data\\ML-Credit-Card-Fraud\\dataloader.py:60: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.28122931 -0.28122931 -0.28122931 ... -0.28122931 -0.28122931\n",
      " -0.28122931]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  data.iloc[:, 1:] = self.scaler.fit_transform(data.iloc[:, 1:])\n"
     ]
    }
   ],
   "source": [
    "import dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "loan = dataloader.LoanNeural()\n",
    "data = loan.get_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = dataloader.split_data(data)\n",
    "\n",
    "# Scale the training data\n",
    "X_train = loan.scale_data(X_train)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(list(zip(X_train, y_train)), batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(list(zip(X_test, y_test)), batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69004029\n",
      "Iteration 2, loss = 0.68628194\n",
      "Iteration 3, loss = 0.68198986\n",
      "Iteration 4, loss = 0.67781988\n",
      "Iteration 5, loss = 0.67362839\n",
      "Iteration 6, loss = 0.66945946\n",
      "Iteration 7, loss = 0.66520008\n",
      "Iteration 8, loss = 0.66073894\n",
      "Iteration 9, loss = 0.65600141\n",
      "Iteration 10, loss = 0.65093263\n",
      "Iteration 11, loss = 0.64564059\n",
      "Iteration 12, loss = 0.63993117\n",
      "Iteration 13, loss = 0.63378585\n",
      "Iteration 14, loss = 0.62727352\n",
      "Iteration 15, loss = 0.62020101\n",
      "Iteration 16, loss = 0.61264673\n",
      "Iteration 17, loss = 0.60460659\n",
      "Iteration 18, loss = 0.59596569\n",
      "Iteration 19, loss = 0.58683940\n",
      "Iteration 20, loss = 0.57714505\n",
      "Iteration 21, loss = 0.56704348\n",
      "Iteration 22, loss = 0.55631285\n",
      "Iteration 23, loss = 0.54528501\n",
      "Iteration 24, loss = 0.53377314\n",
      "Iteration 25, loss = 0.52206787\n",
      "Iteration 26, loss = 0.51026120\n",
      "Iteration 27, loss = 0.49834428\n",
      "Iteration 28, loss = 0.48669975\n",
      "Iteration 29, loss = 0.47515816\n",
      "Iteration 30, loss = 0.46389076\n",
      "Iteration 31, loss = 0.45296786\n",
      "Iteration 32, loss = 0.44249028\n",
      "Iteration 33, loss = 0.43273288\n",
      "Iteration 34, loss = 0.42328774\n",
      "Iteration 35, loss = 0.41446252\n",
      "Iteration 36, loss = 0.40614659\n",
      "Iteration 37, loss = 0.39890121\n",
      "Iteration 38, loss = 0.39165171\n",
      "Iteration 39, loss = 0.38487838\n",
      "Iteration 40, loss = 0.37854018\n",
      "Iteration 41, loss = 0.37288228\n",
      "Iteration 42, loss = 0.36746615\n",
      "Iteration 43, loss = 0.36281207\n",
      "Iteration 44, loss = 0.35804487\n",
      "Iteration 45, loss = 0.35383355\n",
      "Iteration 46, loss = 0.35006217\n",
      "Iteration 47, loss = 0.34635713\n",
      "Iteration 48, loss = 0.34307500\n",
      "Iteration 49, loss = 0.34003373\n",
      "Iteration 50, loss = 0.33697544\n",
      "Iteration 51, loss = 0.33428486\n",
      "Iteration 52, loss = 0.33169996\n",
      "Iteration 53, loss = 0.32945843\n",
      "Iteration 54, loss = 0.32714470\n",
      "Iteration 55, loss = 0.32508249\n",
      "Iteration 56, loss = 0.32316748\n",
      "Iteration 57, loss = 0.32143483\n",
      "Iteration 58, loss = 0.31929746\n",
      "Iteration 59, loss = 0.31757187\n",
      "Iteration 60, loss = 0.31603396\n",
      "Iteration 61, loss = 0.31446987\n",
      "Iteration 62, loss = 0.31302233\n",
      "Iteration 63, loss = 0.31155311\n",
      "Iteration 64, loss = 0.31002434\n",
      "Iteration 65, loss = 0.30858324\n",
      "Iteration 66, loss = 0.30715264\n",
      "Iteration 67, loss = 0.30591649\n",
      "Iteration 68, loss = 0.30444190\n",
      "Iteration 69, loss = 0.30336412\n",
      "Iteration 70, loss = 0.30192038\n",
      "Iteration 71, loss = 0.30080089\n",
      "Iteration 72, loss = 0.29955922\n",
      "Iteration 73, loss = 0.29830229\n",
      "Iteration 74, loss = 0.29740159\n",
      "Iteration 75, loss = 0.29637071\n",
      "Iteration 76, loss = 0.29516205\n",
      "Iteration 77, loss = 0.29405218\n",
      "Iteration 78, loss = 0.29296327\n",
      "Iteration 79, loss = 0.29160900\n",
      "Iteration 80, loss = 0.29072271\n",
      "Iteration 81, loss = 0.28974175\n",
      "Iteration 82, loss = 0.28868289\n",
      "Iteration 83, loss = 0.28773632\n",
      "Iteration 84, loss = 0.28655923\n",
      "Iteration 85, loss = 0.28570360\n",
      "Iteration 86, loss = 0.28469707\n",
      "Iteration 87, loss = 0.28373527\n",
      "Iteration 88, loss = 0.28267459\n",
      "Iteration 89, loss = 0.28185493\n",
      "Iteration 90, loss = 0.28076614\n",
      "Iteration 91, loss = 0.27978744\n",
      "Iteration 92, loss = 0.27886933\n",
      "Iteration 93, loss = 0.27806707\n",
      "Iteration 94, loss = 0.27703275\n",
      "Iteration 95, loss = 0.27611143\n",
      "Iteration 96, loss = 0.27526541\n",
      "Iteration 97, loss = 0.27458018\n",
      "Iteration 98, loss = 0.27344981\n",
      "Iteration 99, loss = 0.27253655\n",
      "Iteration 100, loss = 0.27190196\n",
      "Iteration 101, loss = 0.27075185\n",
      "Iteration 102, loss = 0.26983263\n",
      "Iteration 103, loss = 0.26901611\n",
      "Iteration 104, loss = 0.26816183\n",
      "Iteration 105, loss = 0.26740085\n",
      "Iteration 106, loss = 0.26658985\n",
      "Iteration 107, loss = 0.26554966\n",
      "Iteration 108, loss = 0.26489215\n",
      "Iteration 109, loss = 0.26432316\n",
      "Iteration 110, loss = 0.26331591\n",
      "Iteration 111, loss = 0.26251814\n",
      "Iteration 112, loss = 0.26169741\n",
      "Iteration 113, loss = 0.26097432\n",
      "Iteration 114, loss = 0.25997033\n",
      "Iteration 115, loss = 0.25944063\n",
      "Iteration 116, loss = 0.25862405\n",
      "Iteration 117, loss = 0.25801221\n",
      "Iteration 118, loss = 0.25703939\n",
      "Iteration 119, loss = 0.25636413\n",
      "Iteration 120, loss = 0.25574043\n",
      "Iteration 121, loss = 0.25485566\n",
      "Iteration 122, loss = 0.25412878\n",
      "Iteration 123, loss = 0.25362046\n",
      "Iteration 124, loss = 0.25293988\n",
      "Iteration 125, loss = 0.25209104\n",
      "Iteration 126, loss = 0.25117642\n",
      "Iteration 127, loss = 0.25052455\n",
      "Iteration 128, loss = 0.24968217\n",
      "Iteration 129, loss = 0.24899889\n",
      "Iteration 130, loss = 0.24827580\n",
      "Iteration 131, loss = 0.24748996\n",
      "Iteration 132, loss = 0.24682760\n",
      "Iteration 133, loss = 0.24609543\n",
      "Iteration 134, loss = 0.24565741\n",
      "Iteration 135, loss = 0.24493993\n",
      "Iteration 136, loss = 0.24418345\n",
      "Iteration 137, loss = 0.24331319\n",
      "Iteration 138, loss = 0.24330427\n",
      "Iteration 139, loss = 0.24219239\n",
      "Iteration 140, loss = 0.24157085\n",
      "Iteration 141, loss = 0.24088888\n",
      "Iteration 142, loss = 0.23991760\n",
      "Iteration 143, loss = 0.23952764\n",
      "Iteration 144, loss = 0.23875206\n",
      "Iteration 145, loss = 0.23813570\n",
      "Iteration 146, loss = 0.23774724\n",
      "Iteration 147, loss = 0.23664776\n",
      "Iteration 148, loss = 0.23613675\n",
      "Iteration 149, loss = 0.23541445\n",
      "Iteration 150, loss = 0.23512958\n",
      "Iteration 151, loss = 0.23430348\n",
      "Iteration 152, loss = 0.23371346\n",
      "Iteration 153, loss = 0.23330300\n",
      "Iteration 154, loss = 0.23238169\n",
      "Iteration 155, loss = 0.23171278\n",
      "Iteration 156, loss = 0.23124993\n",
      "Iteration 157, loss = 0.23053157\n",
      "Iteration 158, loss = 0.23019889\n",
      "Iteration 159, loss = 0.22931006\n",
      "Iteration 160, loss = 0.22867300\n",
      "Iteration 161, loss = 0.22857565\n",
      "Iteration 162, loss = 0.22774429\n",
      "Iteration 163, loss = 0.22709969\n",
      "Iteration 164, loss = 0.22646284\n",
      "Iteration 165, loss = 0.22619180\n",
      "Iteration 166, loss = 0.22526894\n",
      "Iteration 167, loss = 0.22480262\n",
      "Iteration 168, loss = 0.22398902\n",
      "Iteration 169, loss = 0.22344178\n",
      "Iteration 170, loss = 0.22260848\n",
      "Iteration 171, loss = 0.22218894\n",
      "Iteration 172, loss = 0.22192551\n",
      "Iteration 173, loss = 0.22107098\n",
      "Iteration 174, loss = 0.22041395\n",
      "Iteration 175, loss = 0.22001756\n",
      "Iteration 176, loss = 0.21928586\n",
      "Iteration 177, loss = 0.21893107\n",
      "Iteration 178, loss = 0.21830007\n",
      "Iteration 179, loss = 0.21766810\n",
      "Iteration 180, loss = 0.21711834\n",
      "Iteration 181, loss = 0.21647470\n",
      "Iteration 182, loss = 0.21609629\n",
      "Iteration 183, loss = 0.21523314\n",
      "Iteration 184, loss = 0.21461303\n",
      "Iteration 185, loss = 0.21435537\n",
      "Iteration 186, loss = 0.21354297\n",
      "Iteration 187, loss = 0.21327736\n",
      "Iteration 188, loss = 0.21250904\n",
      "Iteration 189, loss = 0.21197970\n",
      "Iteration 190, loss = 0.21156380\n",
      "Iteration 191, loss = 0.21072147\n",
      "Iteration 192, loss = 0.21015827\n",
      "Iteration 193, loss = 0.20971897\n",
      "Iteration 194, loss = 0.20919283\n",
      "Iteration 195, loss = 0.20857325\n",
      "Iteration 196, loss = 0.20792437\n",
      "Iteration 197, loss = 0.20740694\n",
      "Iteration 198, loss = 0.20692716\n",
      "Iteration 199, loss = 0.20625784\n",
      "Iteration 200, loss = 0.20576544\n",
      "Iteration 201, loss = 0.20537951\n",
      "Iteration 202, loss = 0.20472029\n",
      "Iteration 203, loss = 0.20400121\n",
      "Iteration 204, loss = 0.20363247\n",
      "Iteration 205, loss = 0.20266449\n",
      "Iteration 206, loss = 0.20231496\n",
      "Iteration 207, loss = 0.20225949\n",
      "Iteration 208, loss = 0.20181085\n",
      "Iteration 209, loss = 0.20112903\n",
      "Iteration 210, loss = 0.20019906\n",
      "Iteration 211, loss = 0.19982128\n",
      "Iteration 212, loss = 0.19913747\n",
      "Iteration 213, loss = 0.19872359\n",
      "Iteration 214, loss = 0.19815088\n",
      "Iteration 215, loss = 0.19764855\n",
      "Iteration 216, loss = 0.19714455\n",
      "Iteration 217, loss = 0.19647353\n",
      "Iteration 218, loss = 0.19576848\n",
      "Iteration 219, loss = 0.19541296\n",
      "Iteration 220, loss = 0.19503399\n",
      "Iteration 221, loss = 0.19439679\n",
      "Iteration 222, loss = 0.19390046\n",
      "Iteration 223, loss = 0.19351334\n",
      "Iteration 224, loss = 0.19277457\n",
      "Iteration 225, loss = 0.19241276\n",
      "Iteration 226, loss = 0.19242381\n",
      "Iteration 227, loss = 0.19113496\n",
      "Iteration 228, loss = 0.19079052\n",
      "Iteration 229, loss = 0.19022661\n",
      "Iteration 230, loss = 0.18992435\n",
      "Iteration 231, loss = 0.18894803\n",
      "Iteration 232, loss = 0.18839236\n",
      "Iteration 233, loss = 0.18807470\n",
      "Iteration 234, loss = 0.18746664\n",
      "Iteration 235, loss = 0.18709622\n",
      "Iteration 236, loss = 0.18665052\n",
      "Iteration 237, loss = 0.18614185\n",
      "Iteration 238, loss = 0.18572411\n",
      "Iteration 239, loss = 0.18486518\n",
      "Iteration 240, loss = 0.18472143\n",
      "Iteration 241, loss = 0.18371876\n",
      "Iteration 242, loss = 0.18351946\n",
      "Iteration 243, loss = 0.18286232\n",
      "Iteration 244, loss = 0.18233308\n",
      "Iteration 245, loss = 0.18176107\n",
      "Iteration 246, loss = 0.18119926\n",
      "Iteration 247, loss = 0.18101313\n",
      "Iteration 248, loss = 0.18031981\n",
      "Iteration 249, loss = 0.17953213\n",
      "Iteration 250, loss = 0.17909187\n",
      "Iteration 251, loss = 0.17888579\n",
      "Iteration 252, loss = 0.17783932\n",
      "Iteration 253, loss = 0.17747343\n",
      "Iteration 254, loss = 0.17731702\n",
      "Iteration 255, loss = 0.17673590\n",
      "Iteration 256, loss = 0.17662562\n",
      "Iteration 257, loss = 0.17549766\n",
      "Iteration 258, loss = 0.17491896\n",
      "Iteration 259, loss = 0.17443010\n",
      "Iteration 260, loss = 0.17390641\n",
      "Iteration 261, loss = 0.17345443\n",
      "Iteration 262, loss = 0.17277706\n",
      "Iteration 263, loss = 0.17289512\n",
      "Iteration 264, loss = 0.17219894\n",
      "Iteration 265, loss = 0.17122497\n",
      "Iteration 266, loss = 0.17130739\n",
      "Iteration 267, loss = 0.17122225\n",
      "Iteration 268, loss = 0.17032287\n",
      "Iteration 269, loss = 0.16946782\n",
      "Iteration 270, loss = 0.16885481\n",
      "Iteration 271, loss = 0.16890215\n",
      "Iteration 272, loss = 0.16808598\n",
      "Iteration 273, loss = 0.16702651\n",
      "Iteration 274, loss = 0.16678855\n",
      "Iteration 275, loss = 0.16606873\n",
      "Iteration 276, loss = 0.16573206\n",
      "Iteration 277, loss = 0.16534988\n",
      "Iteration 278, loss = 0.16459924\n",
      "Iteration 279, loss = 0.16434448\n",
      "Iteration 280, loss = 0.16360396\n",
      "Iteration 281, loss = 0.16335697\n",
      "Iteration 282, loss = 0.16280404\n",
      "Iteration 283, loss = 0.16200955\n",
      "Iteration 284, loss = 0.16169819\n",
      "Iteration 285, loss = 0.16101573\n",
      "Iteration 286, loss = 0.16066380\n",
      "Iteration 287, loss = 0.16019051\n",
      "Iteration 288, loss = 0.15961558\n",
      "Iteration 289, loss = 0.15921077\n",
      "Iteration 290, loss = 0.15827128\n",
      "Iteration 291, loss = 0.15787257\n",
      "Iteration 292, loss = 0.15739318\n",
      "Iteration 293, loss = 0.15723206\n",
      "Iteration 294, loss = 0.15644690\n",
      "Iteration 295, loss = 0.15596304\n",
      "Iteration 296, loss = 0.15509589\n",
      "Iteration 297, loss = 0.15494148\n",
      "Iteration 298, loss = 0.15440180\n",
      "Iteration 299, loss = 0.15406414\n",
      "Iteration 300, loss = 0.15324010\n",
      "Iteration 301, loss = 0.15255105\n",
      "Iteration 302, loss = 0.15255351\n",
      "Iteration 303, loss = 0.15165239\n",
      "Iteration 304, loss = 0.15123986\n",
      "Iteration 305, loss = 0.15063114\n",
      "Iteration 306, loss = 0.15014740\n",
      "Iteration 307, loss = 0.15000712\n",
      "Iteration 308, loss = 0.14950587\n",
      "Iteration 309, loss = 0.14836554\n",
      "Iteration 310, loss = 0.14804296\n",
      "Iteration 311, loss = 0.14790829\n",
      "Iteration 312, loss = 0.14726948\n",
      "Iteration 313, loss = 0.14649824\n",
      "Iteration 314, loss = 0.14614213\n",
      "Iteration 315, loss = 0.14528812\n",
      "Iteration 316, loss = 0.14517069\n",
      "Iteration 317, loss = 0.14518814\n",
      "Iteration 318, loss = 0.14406405\n",
      "Iteration 319, loss = 0.14402048\n",
      "Iteration 320, loss = 0.14315053\n",
      "Iteration 321, loss = 0.14257768\n",
      "Iteration 322, loss = 0.14263841\n",
      "Iteration 323, loss = 0.14167513\n",
      "Iteration 324, loss = 0.14104467\n",
      "Iteration 325, loss = 0.14036005\n",
      "Iteration 326, loss = 0.14008778\n",
      "Iteration 327, loss = 0.13975811\n",
      "Iteration 328, loss = 0.13926220\n",
      "Iteration 329, loss = 0.13849895\n",
      "Iteration 330, loss = 0.13781022\n",
      "Iteration 331, loss = 0.13728076\n",
      "Iteration 332, loss = 0.13697432\n",
      "Iteration 333, loss = 0.13649068\n",
      "Iteration 334, loss = 0.13582820\n",
      "Iteration 335, loss = 0.13519829\n",
      "Iteration 336, loss = 0.13514333\n",
      "Iteration 337, loss = 0.13455567\n",
      "Iteration 338, loss = 0.13437983\n",
      "Iteration 339, loss = 0.13350669\n",
      "Iteration 340, loss = 0.13333776\n",
      "Iteration 341, loss = 0.13261973\n",
      "Iteration 342, loss = 0.13242378\n",
      "Iteration 343, loss = 0.13153620\n",
      "Iteration 344, loss = 0.13119837\n",
      "Iteration 345, loss = 0.13107025\n",
      "Iteration 346, loss = 0.13002953\n",
      "Iteration 347, loss = 0.12975137\n",
      "Iteration 348, loss = 0.12969338\n",
      "Iteration 349, loss = 0.12902997\n",
      "Iteration 350, loss = 0.12859142\n",
      "Iteration 351, loss = 0.12781906\n",
      "Iteration 352, loss = 0.12737243\n",
      "Iteration 353, loss = 0.12678482\n",
      "Iteration 354, loss = 0.12647717\n",
      "Iteration 355, loss = 0.12557682\n",
      "Iteration 356, loss = 0.12550660\n",
      "Iteration 357, loss = 0.12512910\n",
      "Iteration 358, loss = 0.12441999\n",
      "Iteration 359, loss = 0.12437009\n",
      "Iteration 360, loss = 0.12425530\n",
      "Iteration 361, loss = 0.12305761\n",
      "Iteration 362, loss = 0.12287649\n",
      "Iteration 363, loss = 0.12315252\n",
      "Iteration 364, loss = 0.12250056\n",
      "Iteration 365, loss = 0.12143272\n",
      "Iteration 366, loss = 0.12062983\n",
      "Iteration 367, loss = 0.12075964\n",
      "Iteration 368, loss = 0.12016775\n",
      "Iteration 369, loss = 0.11960396\n",
      "Iteration 370, loss = 0.11927727\n",
      "Iteration 371, loss = 0.11878734\n",
      "Iteration 372, loss = 0.11823675\n",
      "Iteration 373, loss = 0.11756065\n",
      "Iteration 374, loss = 0.11723716\n",
      "Iteration 375, loss = 0.11743706\n",
      "Iteration 376, loss = 0.11659481\n",
      "Iteration 377, loss = 0.11580175\n",
      "Iteration 378, loss = 0.11571487\n",
      "Iteration 379, loss = 0.11519070\n",
      "Iteration 380, loss = 0.11423365\n",
      "Iteration 381, loss = 0.11395703\n",
      "Iteration 382, loss = 0.11339729\n",
      "Iteration 383, loss = 0.11333987\n",
      "Iteration 384, loss = 0.11273493\n",
      "Iteration 385, loss = 0.11225415\n",
      "Iteration 386, loss = 0.11203316\n",
      "Iteration 387, loss = 0.11195721\n",
      "Iteration 388, loss = 0.11127916\n",
      "Iteration 389, loss = 0.11095592\n",
      "Iteration 390, loss = 0.11019276\n",
      "Iteration 391, loss = 0.10985145\n",
      "Iteration 392, loss = 0.10937623\n",
      "Iteration 393, loss = 0.10918720\n",
      "Iteration 394, loss = 0.10840879\n",
      "Iteration 395, loss = 0.10785041\n",
      "Iteration 396, loss = 0.10788988\n",
      "Iteration 397, loss = 0.10697255\n",
      "Iteration 398, loss = 0.10693214\n",
      "Iteration 399, loss = 0.10652981\n",
      "Iteration 400, loss = 0.10609453\n",
      "Iteration 401, loss = 0.10535347\n",
      "Iteration 402, loss = 0.10545189\n",
      "Iteration 403, loss = 0.10481084\n",
      "Iteration 404, loss = 0.10498971\n",
      "Iteration 405, loss = 0.10424750\n",
      "Iteration 406, loss = 0.10385931\n",
      "Iteration 407, loss = 0.10320184\n",
      "Iteration 408, loss = 0.10323074\n",
      "Iteration 409, loss = 0.10252207\n",
      "Iteration 410, loss = 0.10233320\n",
      "Iteration 411, loss = 0.10168478\n",
      "Iteration 412, loss = 0.10142152\n",
      "Iteration 413, loss = 0.10070024\n",
      "Iteration 414, loss = 0.10026128\n",
      "Iteration 415, loss = 0.10011917\n",
      "Iteration 416, loss = 0.09948146\n",
      "Iteration 417, loss = 0.09919295\n",
      "Iteration 418, loss = 0.09939772\n",
      "Iteration 419, loss = 0.09877425\n",
      "Iteration 420, loss = 0.09774671\n",
      "Iteration 421, loss = 0.09778668\n",
      "Iteration 422, loss = 0.09742858\n",
      "Iteration 423, loss = 0.09662458\n",
      "Iteration 424, loss = 0.09616430\n",
      "Iteration 425, loss = 0.09579931\n",
      "Iteration 426, loss = 0.09543068\n",
      "Iteration 427, loss = 0.09513559\n",
      "Iteration 428, loss = 0.09472423\n",
      "Iteration 429, loss = 0.09444470\n",
      "Iteration 430, loss = 0.09503699\n",
      "Iteration 431, loss = 0.09343209\n",
      "Iteration 432, loss = 0.09273390\n",
      "Iteration 433, loss = 0.09298684\n",
      "Iteration 434, loss = 0.09230402\n",
      "Iteration 435, loss = 0.09218069\n",
      "Iteration 436, loss = 0.09131697\n",
      "Iteration 437, loss = 0.09119900\n",
      "Iteration 438, loss = 0.09038740\n",
      "Iteration 439, loss = 0.09044628\n",
      "Iteration 440, loss = 0.09059451\n",
      "Iteration 441, loss = 0.08990588\n",
      "Iteration 442, loss = 0.08965838\n",
      "Iteration 443, loss = 0.08934368\n",
      "Iteration 444, loss = 0.08895822\n",
      "Iteration 445, loss = 0.08852489\n",
      "Iteration 446, loss = 0.08842536\n",
      "Iteration 447, loss = 0.08824186\n",
      "Iteration 448, loss = 0.08721529\n",
      "Iteration 449, loss = 0.08727266\n",
      "Iteration 450, loss = 0.08637827\n",
      "Iteration 451, loss = 0.08616083\n",
      "Iteration 452, loss = 0.08590339\n",
      "Iteration 453, loss = 0.08529843\n",
      "Iteration 454, loss = 0.08506174\n",
      "Iteration 455, loss = 0.08481578\n",
      "Iteration 456, loss = 0.08454812\n",
      "Iteration 457, loss = 0.08399824\n",
      "Iteration 458, loss = 0.08329444\n",
      "Iteration 459, loss = 0.08361905\n",
      "Iteration 460, loss = 0.08352141\n",
      "Iteration 461, loss = 0.08277637\n",
      "Iteration 462, loss = 0.08248694\n",
      "Iteration 463, loss = 0.08127014\n",
      "Iteration 464, loss = 0.08227322\n",
      "Iteration 465, loss = 0.08121703\n",
      "Iteration 466, loss = 0.08123273\n",
      "Iteration 467, loss = 0.08080035\n",
      "Iteration 468, loss = 0.07999744\n",
      "Iteration 469, loss = 0.07935361\n",
      "Iteration 470, loss = 0.08007294\n",
      "Iteration 471, loss = 0.07871778\n",
      "Iteration 472, loss = 0.07848081\n",
      "Iteration 473, loss = 0.07818244\n",
      "Iteration 474, loss = 0.07757956\n",
      "Iteration 475, loss = 0.07745045\n",
      "Iteration 476, loss = 0.07719417\n",
      "Iteration 477, loss = 0.07689033\n",
      "Iteration 478, loss = 0.07709328\n",
      "Iteration 479, loss = 0.07631391\n",
      "Iteration 480, loss = 0.07564262\n",
      "Iteration 481, loss = 0.07555115\n",
      "Iteration 482, loss = 0.07498787\n",
      "Iteration 483, loss = 0.07579680\n",
      "Iteration 484, loss = 0.07466452\n",
      "Iteration 485, loss = 0.07462935\n",
      "Iteration 486, loss = 0.07375717\n",
      "Iteration 487, loss = 0.07328711\n",
      "Iteration 488, loss = 0.07297707\n",
      "Iteration 489, loss = 0.07262217\n",
      "Iteration 490, loss = 0.07292617\n",
      "Iteration 491, loss = 0.07240441\n",
      "Iteration 492, loss = 0.07161607\n",
      "Iteration 493, loss = 0.07227802\n",
      "Iteration 494, loss = 0.07142162\n",
      "Iteration 495, loss = 0.07072532\n",
      "Iteration 496, loss = 0.07068136\n",
      "Iteration 497, loss = 0.07044118\n",
      "Iteration 498, loss = 0.06976391\n",
      "Iteration 499, loss = 0.06990053\n",
      "Iteration 500, loss = 0.06938362\n",
      "Iteration 501, loss = 0.06909527\n",
      "Iteration 502, loss = 0.06850945\n",
      "Iteration 503, loss = 0.06880456\n",
      "Iteration 504, loss = 0.06809266\n",
      "Iteration 505, loss = 0.06784655\n",
      "Iteration 506, loss = 0.06744239\n",
      "Iteration 507, loss = 0.06676126\n",
      "Iteration 508, loss = 0.06710729\n",
      "Iteration 509, loss = 0.06647855\n",
      "Iteration 510, loss = 0.06613755\n",
      "Iteration 511, loss = 0.06652302\n",
      "Iteration 512, loss = 0.06555324\n",
      "Iteration 513, loss = 0.06490860\n",
      "Iteration 514, loss = 0.06544078\n",
      "Iteration 515, loss = 0.06503963\n",
      "Iteration 516, loss = 0.06426141\n",
      "Iteration 517, loss = 0.06408671\n",
      "Iteration 518, loss = 0.06394869\n",
      "Iteration 519, loss = 0.06359437\n",
      "Iteration 520, loss = 0.06274875\n",
      "Iteration 521, loss = 0.06273311\n",
      "Iteration 522, loss = 0.06249123\n",
      "Iteration 523, loss = 0.06208403\n",
      "Iteration 524, loss = 0.06223199\n",
      "Iteration 525, loss = 0.06184994\n",
      "Iteration 526, loss = 0.06118243\n",
      "Iteration 527, loss = 0.06108306\n",
      "Iteration 528, loss = 0.06076629\n",
      "Iteration 529, loss = 0.06068940\n",
      "Iteration 530, loss = 0.06057664\n",
      "Iteration 531, loss = 0.06047080\n",
      "Iteration 532, loss = 0.05972507\n",
      "Iteration 533, loss = 0.05977384\n",
      "Iteration 534, loss = 0.06006153\n",
      "Iteration 535, loss = 0.05927316\n",
      "Iteration 536, loss = 0.05897967\n",
      "Iteration 537, loss = 0.05870665\n",
      "Iteration 538, loss = 0.05789263\n",
      "Iteration 539, loss = 0.05779923\n",
      "Iteration 540, loss = 0.05759703\n",
      "Iteration 541, loss = 0.05730187\n",
      "Iteration 542, loss = 0.05763696\n",
      "Iteration 543, loss = 0.05883942\n",
      "Iteration 544, loss = 0.05667407\n",
      "Iteration 545, loss = 0.05639137\n",
      "Iteration 546, loss = 0.05598434\n",
      "Iteration 547, loss = 0.05562263\n",
      "Iteration 548, loss = 0.05566050\n",
      "Iteration 549, loss = 0.05527095\n",
      "Iteration 550, loss = 0.05508693\n",
      "Iteration 551, loss = 0.05449198\n",
      "Iteration 552, loss = 0.05438715\n",
      "Iteration 553, loss = 0.05426155\n",
      "Iteration 554, loss = 0.05390901\n",
      "Iteration 555, loss = 0.05357571\n",
      "Iteration 556, loss = 0.05381283\n",
      "Iteration 557, loss = 0.05340462\n",
      "Iteration 558, loss = 0.05290573\n",
      "Iteration 559, loss = 0.05276728\n",
      "Iteration 560, loss = 0.05279865\n",
      "Iteration 561, loss = 0.05339500\n",
      "Iteration 562, loss = 0.05232495\n",
      "Iteration 563, loss = 0.05186528\n",
      "Iteration 564, loss = 0.05162618\n",
      "Iteration 565, loss = 0.05141499\n",
      "Iteration 566, loss = 0.05091443\n",
      "Iteration 567, loss = 0.05040979\n",
      "Iteration 568, loss = 0.05068830\n",
      "Iteration 569, loss = 0.05018244\n",
      "Iteration 570, loss = 0.05073521\n",
      "Iteration 571, loss = 0.05007116\n",
      "Iteration 572, loss = 0.04966961\n",
      "Iteration 573, loss = 0.04903964\n",
      "Iteration 574, loss = 0.04880406\n",
      "Iteration 575, loss = 0.04897638\n",
      "Iteration 576, loss = 0.04871881\n",
      "Iteration 577, loss = 0.04844183\n",
      "Iteration 578, loss = 0.04803315\n",
      "Iteration 579, loss = 0.04798371\n",
      "Iteration 580, loss = 0.04776813\n",
      "Iteration 581, loss = 0.04748622\n",
      "Iteration 582, loss = 0.04729342\n",
      "Iteration 583, loss = 0.04709246\n",
      "Iteration 584, loss = 0.04685365\n",
      "Iteration 585, loss = 0.04696707\n",
      "Iteration 586, loss = 0.04633310\n",
      "Iteration 587, loss = 0.04652754\n",
      "Iteration 588, loss = 0.04602621\n",
      "Iteration 589, loss = 0.04554906\n",
      "Iteration 590, loss = 0.04541235\n",
      "Iteration 591, loss = 0.04526310\n",
      "Iteration 592, loss = 0.04571726\n",
      "Iteration 593, loss = 0.04506074\n",
      "Iteration 594, loss = 0.04539777\n",
      "Iteration 595, loss = 0.04477875\n",
      "Iteration 596, loss = 0.04416871\n",
      "Iteration 597, loss = 0.04438350\n",
      "Iteration 598, loss = 0.04418928\n",
      "Iteration 599, loss = 0.04390740\n",
      "Iteration 600, loss = 0.04332226\n",
      "Iteration 601, loss = 0.04356032\n",
      "Iteration 602, loss = 0.04330526\n",
      "Iteration 603, loss = 0.04330636\n",
      "Iteration 604, loss = 0.04278183\n",
      "Iteration 605, loss = 0.04290622\n",
      "Iteration 606, loss = 0.04239314\n",
      "Iteration 607, loss = 0.04189772\n",
      "Iteration 608, loss = 0.04192550\n",
      "Iteration 609, loss = 0.04191312\n",
      "Iteration 610, loss = 0.04153618\n",
      "Iteration 611, loss = 0.04120948\n",
      "Iteration 612, loss = 0.04124359\n",
      "Iteration 613, loss = 0.04041848\n",
      "Iteration 614, loss = 0.04063134\n",
      "Iteration 615, loss = 0.04082560\n",
      "Iteration 616, loss = 0.04063518\n",
      "Iteration 617, loss = 0.03998622\n",
      "Iteration 618, loss = 0.03993018\n",
      "Iteration 619, loss = 0.03976976\n",
      "Iteration 620, loss = 0.03980871\n",
      "Iteration 621, loss = 0.03903535\n",
      "Iteration 622, loss = 0.03921381\n",
      "Iteration 623, loss = 0.03893757\n",
      "Iteration 624, loss = 0.03839766\n",
      "Iteration 625, loss = 0.03870035\n",
      "Iteration 626, loss = 0.03856185\n",
      "Iteration 627, loss = 0.03780666\n",
      "Iteration 628, loss = 0.03803881\n",
      "Iteration 629, loss = 0.03825148\n",
      "Iteration 630, loss = 0.03739748\n",
      "Iteration 631, loss = 0.03755287\n",
      "Iteration 632, loss = 0.03714081\n",
      "Iteration 633, loss = 0.03682486\n",
      "Iteration 634, loss = 0.03695236\n",
      "Iteration 635, loss = 0.03683012\n",
      "Iteration 636, loss = 0.03633963\n",
      "Iteration 637, loss = 0.03603540\n",
      "Iteration 638, loss = 0.03603850\n",
      "Iteration 639, loss = 0.03592725\n",
      "Iteration 640, loss = 0.03590521\n",
      "Iteration 641, loss = 0.03558342\n",
      "Iteration 642, loss = 0.03532920\n",
      "Iteration 643, loss = 0.03517011\n",
      "Iteration 644, loss = 0.03476871\n",
      "Iteration 645, loss = 0.03484763\n",
      "Iteration 646, loss = 0.03483260\n",
      "Iteration 647, loss = 0.03466637\n",
      "Iteration 648, loss = 0.03418111\n",
      "Iteration 649, loss = 0.03392419\n",
      "Iteration 650, loss = 0.03413528\n",
      "Iteration 651, loss = 0.03376817\n",
      "Iteration 652, loss = 0.03358125\n",
      "Iteration 653, loss = 0.03341302\n",
      "Iteration 654, loss = 0.03327311\n",
      "Iteration 655, loss = 0.03324063\n",
      "Iteration 656, loss = 0.03272926\n",
      "Iteration 657, loss = 0.03283846\n",
      "Iteration 658, loss = 0.03245090\n",
      "Iteration 659, loss = 0.03244984\n",
      "Iteration 660, loss = 0.03213542\n",
      "Iteration 661, loss = 0.03241638\n",
      "Iteration 662, loss = 0.03193644\n",
      "Iteration 663, loss = 0.03189992\n",
      "Iteration 664, loss = 0.03196371\n",
      "Iteration 665, loss = 0.03163367\n",
      "Iteration 666, loss = 0.03192398\n",
      "Iteration 667, loss = 0.03127592\n",
      "Iteration 668, loss = 0.03113741\n",
      "Iteration 669, loss = 0.03098389\n",
      "Iteration 670, loss = 0.03076052\n",
      "Iteration 671, loss = 0.03077243\n",
      "Iteration 672, loss = 0.03060740\n",
      "Iteration 673, loss = 0.03079865\n",
      "Iteration 674, loss = 0.03039317\n",
      "Iteration 675, loss = 0.03027618\n",
      "Iteration 676, loss = 0.03008491\n",
      "Iteration 677, loss = 0.02981182\n",
      "Iteration 678, loss = 0.02929063\n",
      "Iteration 679, loss = 0.02926113\n",
      "Iteration 680, loss = 0.02938614\n",
      "Iteration 681, loss = 0.02954920\n",
      "Iteration 682, loss = 0.02923379\n",
      "Iteration 683, loss = 0.02877375\n",
      "Iteration 684, loss = 0.02892591\n",
      "Iteration 685, loss = 0.02887718\n",
      "Iteration 686, loss = 0.02842755\n",
      "Iteration 687, loss = 0.02829675\n",
      "Iteration 688, loss = 0.02831253\n",
      "Iteration 689, loss = 0.02828549\n",
      "Iteration 690, loss = 0.02835744\n",
      "Iteration 691, loss = 0.02822495\n",
      "Iteration 692, loss = 0.02754882\n",
      "Iteration 693, loss = 0.02760478\n",
      "Iteration 694, loss = 0.02716677\n",
      "Iteration 695, loss = 0.02696666\n",
      "Iteration 696, loss = 0.02760628\n",
      "Iteration 697, loss = 0.02706208\n",
      "Iteration 698, loss = 0.02666510\n",
      "Iteration 699, loss = 0.02668226\n",
      "Iteration 700, loss = 0.02678454\n",
      "Iteration 701, loss = 0.02683977\n",
      "Iteration 702, loss = 0.02634488\n",
      "Iteration 703, loss = 0.02645233\n",
      "Iteration 704, loss = 0.02629903\n",
      "Iteration 705, loss = 0.02618859\n",
      "Iteration 706, loss = 0.02592279\n",
      "Iteration 707, loss = 0.02577872\n",
      "Iteration 708, loss = 0.02539593\n",
      "Iteration 709, loss = 0.02568870\n",
      "Iteration 710, loss = 0.02520066\n",
      "Iteration 711, loss = 0.02525248\n",
      "Iteration 712, loss = 0.02498451\n",
      "Iteration 713, loss = 0.02516165\n",
      "Iteration 714, loss = 0.02489931\n",
      "Iteration 715, loss = 0.02462744\n",
      "Iteration 716, loss = 0.02461957\n",
      "Iteration 717, loss = 0.02448948\n",
      "Iteration 718, loss = 0.02456421\n",
      "Iteration 719, loss = 0.02407703\n",
      "Iteration 720, loss = 0.02401180\n",
      "Iteration 721, loss = 0.02386744\n",
      "Iteration 722, loss = 0.02413304\n",
      "Iteration 723, loss = 0.02401691\n",
      "Iteration 724, loss = 0.02371436\n",
      "Iteration 725, loss = 0.02361574\n",
      "Iteration 726, loss = 0.02318277\n",
      "Iteration 727, loss = 0.02315486\n",
      "Iteration 728, loss = 0.02338662\n",
      "Iteration 729, loss = 0.02339510\n",
      "Iteration 730, loss = 0.02285149\n",
      "Iteration 731, loss = 0.02281898\n",
      "Iteration 732, loss = 0.02257765\n",
      "Iteration 733, loss = 0.02248587\n",
      "Iteration 734, loss = 0.02266296\n",
      "Iteration 735, loss = 0.02233471\n",
      "Iteration 736, loss = 0.02230379\n",
      "Iteration 737, loss = 0.02213826\n",
      "Iteration 738, loss = 0.02189084\n",
      "Iteration 739, loss = 0.02174849\n",
      "Iteration 740, loss = 0.02166753\n",
      "Iteration 741, loss = 0.02182478\n",
      "Iteration 742, loss = 0.02166365\n",
      "Iteration 743, loss = 0.02177375\n",
      "Iteration 744, loss = 0.02136193\n",
      "Iteration 745, loss = 0.02149399\n",
      "Iteration 746, loss = 0.02113707\n",
      "Iteration 747, loss = 0.02107194\n",
      "Iteration 748, loss = 0.02076410\n",
      "Iteration 749, loss = 0.02081917\n",
      "Iteration 750, loss = 0.02130852\n",
      "Iteration 751, loss = 0.02080232\n",
      "Iteration 752, loss = 0.02051762\n",
      "Iteration 753, loss = 0.02028995\n",
      "Iteration 754, loss = 0.02023100\n",
      "Iteration 755, loss = 0.02016285\n",
      "Iteration 756, loss = 0.02020948\n",
      "Iteration 757, loss = 0.02077772\n",
      "Iteration 758, loss = 0.02001198\n",
      "Iteration 759, loss = 0.02003353\n",
      "Iteration 760, loss = 0.01972467\n",
      "Iteration 761, loss = 0.01962632\n",
      "Iteration 762, loss = 0.01975623\n",
      "Iteration 763, loss = 0.01934701\n",
      "Iteration 764, loss = 0.01949242\n",
      "Iteration 765, loss = 0.01928772\n",
      "Iteration 766, loss = 0.01919665\n",
      "Iteration 767, loss = 0.01967742\n",
      "Iteration 768, loss = 0.01916843\n",
      "Iteration 769, loss = 0.01893536\n",
      "Iteration 770, loss = 0.01896787\n",
      "Iteration 771, loss = 0.01873317\n",
      "Iteration 772, loss = 0.01882515\n",
      "Iteration 773, loss = 0.01851391\n",
      "Iteration 774, loss = 0.01840030\n",
      "Iteration 775, loss = 0.01849131\n",
      "Iteration 776, loss = 0.01832892\n",
      "Iteration 777, loss = 0.01831533\n",
      "Iteration 778, loss = 0.01831007\n",
      "Iteration 779, loss = 0.01805730\n",
      "Iteration 780, loss = 0.01792437\n",
      "Iteration 781, loss = 0.01773735\n",
      "Iteration 782, loss = 0.01790584\n",
      "Iteration 783, loss = 0.01759455\n",
      "Iteration 784, loss = 0.01760757\n",
      "Iteration 785, loss = 0.01747540\n",
      "Iteration 786, loss = 0.01729562\n",
      "Iteration 787, loss = 0.01703432\n",
      "Iteration 788, loss = 0.01689464\n",
      "Iteration 789, loss = 0.01746184\n",
      "Iteration 790, loss = 0.01740939\n",
      "Iteration 791, loss = 0.01687925\n",
      "Iteration 792, loss = 0.01685905\n",
      "Iteration 793, loss = 0.01663281\n",
      "Iteration 794, loss = 0.01679990\n",
      "Iteration 795, loss = 0.01664174\n",
      "Iteration 796, loss = 0.01652600\n",
      "Iteration 797, loss = 0.01640912\n",
      "Iteration 798, loss = 0.01647726\n",
      "Iteration 799, loss = 0.01604200\n",
      "Iteration 800, loss = 0.01672603\n",
      "Iteration 801, loss = 0.01617985\n",
      "Iteration 802, loss = 0.01590103\n",
      "Iteration 803, loss = 0.01614885\n",
      "Iteration 804, loss = 0.01651971\n",
      "Iteration 805, loss = 0.01574956\n",
      "Iteration 806, loss = 0.01562036\n",
      "Iteration 807, loss = 0.01561523\n",
      "Iteration 808, loss = 0.01565997\n",
      "Iteration 809, loss = 0.01552597\n",
      "Iteration 810, loss = 0.01562088\n",
      "Iteration 811, loss = 0.01557832\n",
      "Iteration 812, loss = 0.01522077\n",
      "Iteration 813, loss = 0.01556021\n",
      "Iteration 814, loss = 0.01515024\n",
      "Iteration 815, loss = 0.01508477\n",
      "Iteration 816, loss = 0.01514177\n",
      "Iteration 817, loss = 0.01508681\n",
      "Iteration 818, loss = 0.01491602\n",
      "Iteration 819, loss = 0.01482532\n",
      "Iteration 820, loss = 0.01461406\n",
      "Iteration 821, loss = 0.01467024\n",
      "Iteration 822, loss = 0.01445967\n",
      "Iteration 823, loss = 0.01468072\n",
      "Iteration 824, loss = 0.01464340\n",
      "Iteration 825, loss = 0.01431752\n",
      "Iteration 826, loss = 0.01417762\n",
      "Iteration 827, loss = 0.01422533\n",
      "Iteration 828, loss = 0.01414151\n",
      "Iteration 829, loss = 0.01450789\n",
      "Iteration 830, loss = 0.01411165\n",
      "Iteration 831, loss = 0.01403904\n",
      "Iteration 832, loss = 0.01425389\n",
      "Iteration 833, loss = 0.01386228\n",
      "Iteration 834, loss = 0.01371434\n",
      "Iteration 835, loss = 0.01385872\n",
      "Iteration 836, loss = 0.01382422\n",
      "Iteration 837, loss = 0.01353134\n",
      "Iteration 838, loss = 0.01350665\n",
      "Iteration 839, loss = 0.01398477\n",
      "Iteration 840, loss = 0.01351840\n",
      "Iteration 841, loss = 0.01329492\n",
      "Iteration 842, loss = 0.01318088\n",
      "Iteration 843, loss = 0.01352970\n",
      "Iteration 844, loss = 0.01294591\n",
      "Iteration 845, loss = 0.01308346\n",
      "Iteration 846, loss = 0.01293206\n",
      "Iteration 847, loss = 0.01334510\n",
      "Iteration 848, loss = 0.01295639\n",
      "Iteration 849, loss = 0.01296053\n",
      "Iteration 850, loss = 0.01277908\n",
      "Iteration 851, loss = 0.01302812\n",
      "Iteration 852, loss = 0.01255203\n",
      "Iteration 853, loss = 0.01259438\n",
      "Iteration 854, loss = 0.01256862\n",
      "Iteration 855, loss = 0.01251458\n",
      "Iteration 856, loss = 0.01228301\n",
      "Iteration 857, loss = 0.01232834\n",
      "Iteration 858, loss = 0.01239316\n",
      "Iteration 859, loss = 0.01210171\n",
      "Iteration 860, loss = 0.01224715\n",
      "Iteration 861, loss = 0.01229791\n",
      "Iteration 862, loss = 0.01212817\n",
      "Iteration 863, loss = 0.01241557\n",
      "Iteration 864, loss = 0.01211554\n",
      "Iteration 865, loss = 0.01183909\n",
      "Iteration 866, loss = 0.01205711\n",
      "Iteration 867, loss = 0.01189142\n",
      "Iteration 868, loss = 0.01177810\n",
      "Iteration 869, loss = 0.01148566\n",
      "Iteration 870, loss = 0.01175317\n",
      "Iteration 871, loss = 0.01179888\n",
      "Iteration 872, loss = 0.01153698\n",
      "Iteration 873, loss = 0.01154750\n",
      "Iteration 874, loss = 0.01156061\n",
      "Iteration 875, loss = 0.01167884\n",
      "Iteration 876, loss = 0.01123632\n",
      "Iteration 877, loss = 0.01141714\n",
      "Iteration 878, loss = 0.01122933\n",
      "Iteration 879, loss = 0.01109133\n",
      "Iteration 880, loss = 0.01143843\n",
      "Iteration 881, loss = 0.01128432\n",
      "Iteration 882, loss = 0.01100635\n",
      "Iteration 883, loss = 0.01107248\n",
      "Iteration 884, loss = 0.01111925\n",
      "Iteration 885, loss = 0.01099368\n",
      "Iteration 886, loss = 0.01102185\n",
      "Iteration 887, loss = 0.01105324\n",
      "Iteration 888, loss = 0.01077091\n",
      "Iteration 889, loss = 0.01069549\n",
      "Iteration 890, loss = 0.01071817\n",
      "Iteration 891, loss = 0.01057957\n",
      "Iteration 892, loss = 0.01085914\n",
      "Iteration 893, loss = 0.01060034\n",
      "Iteration 894, loss = 0.01064846\n",
      "Iteration 895, loss = 0.01050124\n",
      "Iteration 896, loss = 0.01068085\n",
      "Iteration 897, loss = 0.01040311\n",
      "Iteration 898, loss = 0.01047410\n",
      "Iteration 899, loss = 0.01040866\n",
      "Iteration 900, loss = 0.01035657\n",
      "Iteration 901, loss = 0.01035785\n",
      "Iteration 902, loss = 0.01036958\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Create an instance of the MLPClassifier\n",
    "model = MLPClassifier(hidden_layer_sizes=(100, 100, 100, 100), activation='relu', solver='sgd', random_state=42, verbose=True, max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(loan.scale_data(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49331550802139035"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.sum() / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9077540106951871\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
